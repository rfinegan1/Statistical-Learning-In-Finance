---
title: "LinearRegressionYields"
author: "Ryan Finegan"
date: "10/30/2021"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(caret)
library(MASS)
library(glmnet)
library(class)
library(ISLR)
library(tidyverse)

setwd("/Users/ryanfinegan/Documents")   # my working directory
df<-read.csv("10yearforecasting.csv")                   # file for 10 year prediction
dates <- as.POSIXct(df$Dates, format = "%m/%d/%Y")      # converting to get just the year
df$Dates<-format(dates, format="%Y")                    # getting the year in dates
df<-df[-1,]                                             # getting rid of the first row zeros
glimpse(df)
```


```{r}
### Simple Linear Regression with move weekly
ten_move <- lm(ten ~ movelag1, data = df)
summary(ten_move)        # p-value is extremely small (significant)
summary(ten_move)$sigma  # RSE of the Model
summary(ten_move)$sigma / mean(df$ten)  # divide it by y to get percentage error
summary(ten_move)$r.squared             # model can explain 16.01 percent of variance in weekly ten year yield
coefficients(ten_move)[2]               # barely a negative relationship (higher move sends 10 year yield lower)

### this function predicts intervals for the ten year yield given 95% confidence interval with certain value of the derivative of the ten year lagged 1 week
predict(ten_move, data.frame(movelag1 = 95.77), interval = "confidence", level = 0.95)
predict(ten_move, data.frame(movelag1 = 95.77), interval = "prediction", level = 0.95) # same thing but for prediction interval
```



```{r}
### Plotting response against predictor
theme_set(theme_light())
ggplot(df, aes(x = movelag1, y = ten)) + 
  geom_point() + 
  geom_abline(intercept = coef(ten_move)[1], slope = coef(ten_move)[2], 
              col = "deepskyblue3", 
              size = 1) + 
  geom_smooth(se = F)    # typical looking plot for deltas
```

```{r}
### Diagnostic Plots
par(mfrow=c(2,2))
plot(ten_move)
```

```{r}
### Multiple Linear Regression 
df.viz = subset(df, select = -c(Dates,thirtyderivative,thirty,movederivative,
                                move,dxyderivative,dxy) ) # getting rid of variables that aren't lagged
df.viz1=subset(df.viz,select=c(ten,tenlag1,tenlag5,movelag1,movelag5,tenderivativelag1,tenderivativelag5,
                              movederivativelag1,movederivativelag5,thirtyderivativelag1,thirtyderivativelag5))
pairs(df.viz1)                # data relationships
cor(df.viz[-1], df.viz$ten)   # correlation with ten year yield with rest of predictors
ten.year.lm <- lm(ten ~ movelag1 + movelag5 + dxylag1, data = df.viz)  # mlr model 
summary(ten.year.lm)          #  movelag1, movelag5, dxylag1

### Diagnostic Plots
par(mfrow=c(2,2))
plot(ten.year.lm)
```

```{r}
### Interaction Effects (Show all Interactions)
summary(lm(formula = ten ~ . * ., data = df.viz))   # tenlag2:movederivativelag5 significance is interesting
```
```{r}
### transformations of the variables, such as log(X), X^2
### Took awesome function from lmorgan95
best_predictor <- function(dataframe, response) {
  if (sum(sapply(dataframe, function(x) {is.numeric(x) | is.factor(x)})) < ncol(dataframe)) {
    stop("Make sure that all variables are of class numeric/factor!")
  }
  # pre-allocate vectors
  varname <- c()
  vartype <- c()
  R2 <- c()
  R2_log <- c()
  R2_quad <- c()
  AIC <- c()
  AIC_log <- c()
  AIC_quad <- c()
  y <- dataframe[ ,response]
  # # # # # NUMERIC RESPONSE # # # # #
  if (is.numeric(y)) {
    for (i in 1:ncol(dataframe)) {
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      if (!identical(y, x)) {
        # linear: y ~ x
        R2[i] <- summary(lm(y ~ x))$r.squared 
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
            R2_log[i] <- summary(lm(y ~ log(x + abs(min(x)) + 1)))$r.squared
          } else {
            R2_log[i] <- summary(lm(y ~ log(x)))$r.squared
          }
        } else {
          R2_log[i] <- NA
        }
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          R2_quad[i] <- summary(lm(y ~ x + I(x^2)))$r.squared
        } else {
          R2_quad[i] <- NA
        }
      } else {
        R2[i] <- NA
        R2_log[i] <- NA
        R2_quad[i] <- NA
      }
    }
    print(paste("Response variable:", response))
    data.frame(varname, 
               vartype, 
               R2 = round(R2, 3), 
               R2_log = round(R2_log, 3), 
               R2_quad = round(R2_quad, 3)) %>%
      mutate(max_R2 = pmax(R2, R2_log, R2_quad, na.rm = T)) %>%
      arrange(desc(max_R2))
    # # # # # CATEGORICAL RESPONSE # # # # #
  } else {
    for (i in 1:ncol(dataframe)) {
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      if (!identical(y, x)) {
        # linear: y ~ x
        AIC[i] <- summary(glm(y ~ x, family = "binomial"))$aic 
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
            AIC_log[i] <- summary(glm(y ~ log(x + abs(min(x)) + 1), family = "binomial"))$aic
          } else {
            AIC_log[i] <- summary(glm(y ~ log(x), family = "binomial"))$aic
          }
        } else {
          AIC_log[i] <- NA
        }
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          AIC_quad[i] <- summary(glm(y ~ x + I(x^2), family = "binomial"))$aic
        } else {
          AIC_quad[i] <- NA
        }
      } else {
        AIC[i] <- NA
        AIC_log[i] <- NA
        AIC_quad[i] <- NA
      }
    }
    print(paste("Response variable:", response))
    data.frame(varname, 
               vartype, 
               AIC = round(AIC, 3), 
               AIC_log = round(AIC_log, 3), 
               AIC_quad = round(AIC_quad, 3)) %>%
      mutate(min_AIC = pmin(AIC, AIC_log, AIC_quad, na.rm = T)) %>%
      arrange(min_AIC)
  } 
}

### Took awesome function from lmorgan95

best_predictor(df.viz, "ten") # movelag2, movelag1, movelag3, movelag4, movelag5 => quad transform
```
```{r}
transform.lm <- lm(ten ~ + I(movelag2^2) + I(movelag1^2) + I(movelag3^2) + I(movelag4^2) + I(movelag5^2), data = df.viz)
summary(transform.lm)    # transform with movelag1
### significance increased for "I(tenlag1^2)", "I(movederivativelag5^2)", and "I(tenlag5^2)"
new.transform.lm <- lm(ten ~ thirtylag1 + I(movelag1^2) + movelag5 + dxylag1, data = df.viz)
summary(new.transform.lm)
### Diagnostic Plots
par(mfrow=c(2,2))
plot(new.transform.lm)
```
```{r}
### Try log transform of Target Variable [The Ten Year Yield]
target.transform=lm(log(ten)~.,data=df.viz)
summary(target.transform)
### log transform on target variable
log.transform.lm <- lm(log(ten) ~ movelag5 + movelag1 + dxylag1, data = df.viz)
summary(log.transform.lm)
```
## \textcolor{red}{Solution:} 
These are the best models from the experiments above
```{r}
### best models with linear regression
best.mod.1 <- lm(ten ~ thirtylag1 + I(movelag1^2) + movelag5 + dxylag1, data = df.viz)
summary(best.mod.1)
best.mod.2 <- lm(log(ten) ~ movelag5 + movelag1 + dxylag1, data = df.viz)
summary(best.mod.2)
```